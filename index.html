<!doctype html>
<html class="no-js" dir="ltr" lang="en">
<head>
    <meta charset="utf-8">
    <meta content="ie=edge" http-equiv="x-ua-compatible">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <link href="css/libs.min.css" rel="stylesheet" type="text/css">
    <link href="css/app.min.css" rel="stylesheet" type="text/css">
    <link href="css/action.min.css" rel="stylesheet" type="text/css">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <title>CochleaNet : Audio-Visual Speech Enhancement</title>
    <meta content="We present CochleaNet, a DNN that exploits the noisy acoustic cues and noise robust visual cues to focus on the desired speaker and improve intelligibility."
          name="Description">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-68974083-8"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-68974083-8');
    </script>
</head>

<body class="home blog">
<header>
    <div class="logo-wrapper">
        <a href="https://cochleanet.github.io">
            <img alt="logo" src="/images/logo.png" style="padding-left:10px; max-height: 50px"/>
        </a>
    </div>
    <div class="menu-wrapper">
        <ul>
            <li><a data-smooth-scroll href="#action">Demo</a></li>
            <li><a href="/supplementary/">Supplementary Material</a></li>
        </ul>
    </div>
    <div class="logo-wrapper-resp">
        <a href="/">
            <img alt="logo" src="/images/logo.png" style="padding-left:10px; max-height: 50px"/>
        </a>
    </div>
    <div id="burger">
        <span></span>
        <span></span>
        <span></span>
    </div>
    <div id="mobile-nav">
        <ul class="mobile-nav">
            <li><a data-smooth-scroll href="#action">Demo</a></li>
            <li><a href="/supplementary/">Supplementary Material</a></li>
        </ul>
    </div>

</header>
<div class="row expanded">
    <h3 class="text-center">
        CochleaNet : A Robust Language-independent Audio-Visual Model for Speech Enhancement<br>
    </h3>
    <h5 class="text-center">
        Mandar Gogate, Kia Dashtipour, Ahsan Adeel, Amir Hussain
    </h5>


    <div class="row text-justify">
      <p>
            Noisy situations cause huge problems for suffers of hearing loss as hearing aids often make the signal more audible but do not always restore the intelligibility. In noisy settings, humans routinely exploit the audio-visual (AV) nature of the speech to selectively suppress the background noise and to focus on the target speaker. In this paper, we present a causal, language, noise and speaker independent AV deep neural network (DNN) architecture for speech enhancement (SE). The model exploits the noisy acoustic cues and noise robust visual cues to focus on the desired speaker and improve the speech intelligibility. To evaluate the proposed SE framework a first of its kind AV binaural speech corpus, called ASPIRE, is recorded in real noisy environments including cafeteria and restaurant. We demonstrate superior performance of our approach in terms of objective measures and subjective listening tests over the state-of-the-art SE approaches as well as recent DNN based SE models. In addition, our work challenges a popular belief that a scarcity of multi-language large vocabulary AV corpus and wide variety of noises is a major bottleneck to build a robust language, speaker and noise independent SE systems. We show that a model trained on synthetic mixture of Grid corpus (with 33 speakers and a small English vocabulary) and ChiME 3 Noises (consisting of only bus, pedestrian, cafeteria, and street noises) generalise well not only on large vocabulary corpora but also on completely unrelated languages (such as Mandarin), wide variety of speakers and noises.
      </p>
        <br>


        <h4 class="text-center">ASPIRE Corpus will be available here shortly<br><br></h4>

        <div class="text-center" id="action">
            <h2 class="text-center wow fadeInDown" data-wow-delay="0.6s">
                See CochleaNet in Action
            </h2>
            <div style="padding:46.48% 0 0 0;position:relative;">
                <iframe title="CochleaNet Enhanced Speech" allow="autoplay; fullscreen" allowfullscreen
                        src="https://player.vimeo.com/video/357546330?title=0&loop=1&byline=0&portrait=0"
                        style="position:absolute;top:0;left:0;width:100%;height:100%;border: none;"></iframe>
            </div>
        </div>

    </div>
</div>


<!--<script crossorigin="anonymous" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ="-->
<!--        src="https://code.jquery.com/jquery-1.12.4.min.js"></script>-->
<script src="https://player.vimeo.com/api/player.js"></script>
<script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-migrate-3.1.0.min.js"></script>
<!--<script defer src="js/main_scripts.js" type="text/javascript"></script>-->
<script type="text/javascript" src="/js/action.js" defer></script>
<!--<script defer src="js/all.min.js" type="text/javascript"></script>-->
<!--<script type="text/javascript" src="js/wow.min.js"></script>-->
<!--<script type="text/javascript" src="js/index.js" defer></script>-->
</body>
</html>
